{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Download the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://github.com/stevelukis/rnn-ind-eng/raw/main/ind-eng.zip\"\n",
    "path = tf.keras.utils.get_file(\"ind-eng.zip\", origin=url, cache_dir=\".\",\n",
    "                               extract=True)\n",
    "text = (Path(path).parent / \"ind.txt\").read_text(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset is tab-formatted like this \"english   indonesian   description\". We only need the English (features) and Indonesian (target)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "triples = [line.split('\\t') for line in text.splitlines()]\n",
    "np.random.shuffle(triples)\n",
    "sentences_en, sentences_id, _ = zip(*triples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is a small dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9243\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_en))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a good question. => Ini pertanyaan yang bagus\n",
      "Is it far from here? => Apakah jauh dari sini?\n",
      "His bag was stolen yesterday. => Tasnya telah dicuri kemarin.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(sentences_en[i] + ' => ' + sentences_id[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vectorizing the text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "vocab_size = 500\n",
    "max_length = 50\n",
    "\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length\n",
    ")\n",
    "text_vec_layer_id = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length\n",
    ")\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_id.adapt([f'startofseq {s} endofseq' for s in sentences_id])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:8500])\n",
    "X_val = tf.constant(sentences_en[8500:])\n",
    "X_train_dec = tf.constant([f'startofseq {s}' for s in sentences_id[:8500]])\n",
    "X_val_dec = tf.constant([f'startofseq {s}' for s in sentences_id[8500:]])\n",
    "\n",
    "y_train = text_vec_layer_id([f'{s} endofseq' for s in sentences_id[:8500]])\n",
    "y_val = text_vec_layer_id([f'{s} endofseq' for s in sentences_id[8500:]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining model structure using Functional API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "embed_size = 128\n",
    "\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_id(decoder_inputs)\n",
    "\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                                    embed_size,\n",
    "                                                    mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                                    embed_size,\n",
    "                                                    mask_zero=True)\n",
    "\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_state=True)\n",
    ")\n",
    "\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),\n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "y_proba = output_layer(decoder_outputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "266/266 [==============================] - 24s 42ms/step - loss: 0.4852 - accuracy: 0.3222 - val_loss: 0.4262 - val_accuracy: 0.3870\n",
      "Epoch 2/20\n",
      "266/266 [==============================] - 8s 29ms/step - loss: 0.4005 - accuracy: 0.4031 - val_loss: 0.3727 - val_accuracy: 0.4304\n",
      "Epoch 3/20\n",
      "266/266 [==============================] - 9s 33ms/step - loss: 0.3541 - accuracy: 0.4467 - val_loss: 0.3419 - val_accuracy: 0.4658\n",
      "Epoch 4/20\n",
      "266/266 [==============================] - 9s 34ms/step - loss: 0.3175 - accuracy: 0.4825 - val_loss: 0.3204 - val_accuracy: 0.4930\n",
      "Epoch 5/20\n",
      "266/266 [==============================] - 10s 36ms/step - loss: 0.2861 - accuracy: 0.5132 - val_loss: 0.3054 - val_accuracy: 0.5087\n",
      "Epoch 6/20\n",
      "266/266 [==============================] - 9s 35ms/step - loss: 0.2576 - accuracy: 0.5434 - val_loss: 0.2940 - val_accuracy: 0.5068\n",
      "Epoch 7/20\n",
      "266/266 [==============================] - 9s 35ms/step - loss: 0.2318 - accuracy: 0.5709 - val_loss: 0.2847 - val_accuracy: 0.5280\n",
      "Epoch 8/20\n",
      "266/266 [==============================] - 9s 35ms/step - loss: 0.2057 - accuracy: 0.6053 - val_loss: 0.2780 - val_accuracy: 0.5385\n",
      "Epoch 9/20\n",
      "266/266 [==============================] - 10s 36ms/step - loss: 0.1810 - accuracy: 0.6408 - val_loss: 0.2758 - val_accuracy: 0.5426\n",
      "Epoch 10/20\n",
      "266/266 [==============================] - 10s 36ms/step - loss: 0.1575 - accuracy: 0.6784 - val_loss: 0.2756 - val_accuracy: 0.5463\n",
      "Epoch 11/20\n",
      "266/266 [==============================] - 10s 36ms/step - loss: 0.1356 - accuracy: 0.7193 - val_loss: 0.2745 - val_accuracy: 0.5467\n",
      "Epoch 12/20\n",
      "266/266 [==============================] - 10s 36ms/step - loss: 0.1145 - accuracy: 0.7595 - val_loss: 0.2814 - val_accuracy: 0.5485\n",
      "Epoch 13/20\n",
      "266/266 [==============================] - 9s 35ms/step - loss: 0.0957 - accuracy: 0.7990 - val_loss: 0.2837 - val_accuracy: 0.5523\n",
      "Epoch 14/20\n",
      "266/266 [==============================] - 9s 35ms/step - loss: 0.0786 - accuracy: 0.8392 - val_loss: 0.2857 - val_accuracy: 0.5513\n",
      "Epoch 15/20\n",
      "266/266 [==============================] - 9s 35ms/step - loss: 0.0645 - accuracy: 0.8710 - val_loss: 0.2921 - val_accuracy: 0.5513\n",
      "Epoch 16/20\n",
      "266/266 [==============================] - 9s 36ms/step - loss: 0.0525 - accuracy: 0.8995 - val_loss: 0.2990 - val_accuracy: 0.5543\n",
      "Epoch 17/20\n",
      "266/266 [==============================] - 9s 36ms/step - loss: 0.0424 - accuracy: 0.9224 - val_loss: 0.3047 - val_accuracy: 0.5526\n",
      "Epoch 18/20\n",
      "266/266 [==============================] - 9s 35ms/step - loss: 0.0350 - accuracy: 0.9364 - val_loss: 0.3093 - val_accuracy: 0.5489\n",
      "Epoch 19/20\n",
      "266/266 [==============================] - 9s 35ms/step - loss: 0.0291 - accuracy: 0.9490 - val_loss: 0.3162 - val_accuracy: 0.5437\n",
      "Epoch 20/20\n",
      "266/266 [==============================] - 10s 37ms/step - loss: 0.0246 - accuracy: 0.9562 - val_loss: 0.3232 - val_accuracy: 0.5482\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[y_proba])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit((X_train, X_train_dec), y_train, epochs=20,\n",
    "                    validation_data=((X_val, X_val_dec), y_val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use word-by-word prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def translate(sentence_en):\n",
    "    translation = ''\n",
    "    for word_idx in range(max_length):\n",
    "        X = np.array([sentence_en])\n",
    "        X_dec = np.array([f'startofseq ' + translation])\n",
    "        y_proba = model.predict([X, X_dec])[0, word_idx]\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_id.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == 'endofseq':\n",
    "            break\n",
    "        translation += ' ' + predicted_word\n",
    "    return translation.strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "'aku pergi ke sekolah untuk [UNK] ini'"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate('I go to school today')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `aku pergi ke sekolah` part is correct. The rest is nonsense..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trying beam search."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def top_k_indices(k, arr):\n",
    "    return np.argsort(arr, axis=0)[-k:][::-1]\n",
    "\n",
    "\n",
    "vocab = text_vec_layer_id.get_vocabulary()\n",
    "\n",
    "\n",
    "def beam_search_translate(sentence_en, k):\n",
    "    X = np.array([sentence_en])\n",
    "    X_dec = np.array(['startofseq'])\n",
    "\n",
    "    y_proba = model.predict([X, X_dec])[0, 0]\n",
    "    top_k_words_id = top_k_indices(k, y_proba)\n",
    "    top_k_translations = [(vocab[word_id], y_proba[word_id]) for word_id in top_k_words_id]\n",
    "\n",
    "    for i in range(1, max_length):\n",
    "        top_branch_translations = []\n",
    "        for translation, proba in top_k_translations:\n",
    "            X_dec = np.array([f'startofseq {translation}'])\n",
    "\n",
    "            y_proba = model.predict([X, X_dec])[0][i]\n",
    "            top_k_words_id = top_k_indices(k, y_proba)\n",
    "            top_branch_translations += [(translation + ' ' + vocab[word_id], y_proba[word_id] * proba)\n",
    "                                        for word_id in top_k_words_id]\n",
    "\n",
    "        top_k_translations = sorted(top_branch_translations, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        for translation, proba in top_k_translations:\n",
    "            if 'endofseq' in translation:\n",
    "                return translation[:-len('endofseq')-1]\n",
    "\n",
    "    return top_k_translations[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "'aku datang ke sekolah besok'"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search_translate('I go to school today', 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, it is still wrong. It says `besok` which means `tomorrow` in English, although we say `today`. But at least it knows that we have time modifier at the end of the sentence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
